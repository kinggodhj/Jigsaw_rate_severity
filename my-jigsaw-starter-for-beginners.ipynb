{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jigsaw Pytorch Starter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases\n",
    "Machine learning experiment tracking, dataset versioning, and model evaluation\n",
    "Weights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues.\n",
    "Set up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-12-02T16:32:40.449864Z",
     "iopub.status.busy": "2021-12-02T16:32:40.449454Z",
     "iopub.status.idle": "2021-12-02T16:32:47.437681Z",
     "shell.execute_reply": "2021-12-02T16:32:47.436834Z",
     "shell.execute_reply.started": "2021-12-02T16:32:40.449828Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb  # Machine learning developer platform for building better models, faster with experiment tracking, dataset versioning, and model management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:32:47.439619Z",
     "iopub.status.busy": "2021-12-02T16:32:47.439381Z",
     "iopub.status.idle": "2021-12-02T16:32:47.447735Z",
     "shell.execute_reply": "2021-12-02T16:32:47.446698Z",
     "shell.execute_reply.started": "2021-12-02T16:32:47.439592Z"
    }
   },
   "outputs": [],
   "source": [
    "import os  # operating system library\n",
    "import gc  # Garbage Collector - module provides the ability to disable the collector, tune the collection frequency, and set debugging options\n",
    "import copy  # The assignment operation does not copy the object, it only creates a reference to the object. \n",
    "# For mutable collections, or for collections containing mutable items, a copy is often needed so that it can be modified without changing the original. \n",
    "# This module provides general (shallow and deep) copy operations. \n",
    "import time  # time library\n",
    "import random  # library for working with random values\n",
    "\n",
    "import string  # common string operations\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd  # data analysis library\n",
    "import numpy as np  # library linear algebra, Fourier transform and random numbers\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch  #  a Tensor library like NumPy, with strong GPU support\n",
    "import torch.nn as nn  # a neural networks library deeply integrated with autograd designed for maximum flexibility\n",
    "import torch.optim as optim  # Pytorch also has a package with various optimization algorithms.\n",
    "# We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.\n",
    "# This will let us replace our previous manually coded optimization step\n",
    "from torch.optim import lr_scheduler  # provides several methods to adjust the learning rate based on the number of epochs.\n",
    "from torch.utils.data import Dataset, DataLoader  # DataLoader and other utility functions for convenience\n",
    "# DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches. \n",
    "# PyTorch has an abstract Dataset class. A Dataset can be anything that has a __len__ function (called by Python’s standard len function) and a __getitem__ function \n",
    "# as a way of indexing into it\n",
    "\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm  # tqdm derives from the Arabic word taqaddum  which can mean \"progress,\" and is an abbreviation for \"I love you so much\" in Spanish \n",
    "# (te quiero demasiado).  this library show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
    "from collections import defaultdict  # Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. \n",
    "# The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). \n",
    "\n",
    "\n",
    "# Sklearn Imports\n",
    "# sklearn - а set of python modules for machine learning and data mining\n",
    "from sklearn.metrics import mean_squared_error  # Mean squared error regression loss\n",
    "from sklearn.model_selection import StratifiedKFold, KFold  # Stratified K-Folds cross-validator.\n",
    "# K-Folds cross-validator. Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "# Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "\n",
    "# For Transformer Models\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW  \n",
    "#  In many cases, the architecture you want to use can be guessed from the name or the path of the \n",
    "# pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the \n",
    "# relevant model given the name/path to the pretrained weights/config/vocabulary.\n",
    "# Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "# ANSI escape character sequences have long been used to produce colored terminal text and cursor positioning on Unix and Macs. \n",
    "# Colorama makes this work on Windows, too, by wrapping stdout, stripping ANSI sequences it finds (which would appear as gobbledygook in the output), \n",
    "# and converting them into the appropriate win32 calls to modify the state of the terminal. On other platforms, Colorama does nothing.\n",
    "\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings  # Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # This is the base class of all warning category classes. It is a subclass of Exception. \n",
    "# The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception) \n",
    "# \"ignore\" - never print matching warnings\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# The beautiful thing of PyTorch's immediate execution model is that you can actually debug your programs. Sometimes, however, the asynchronous nature \n",
    "# of CUDA execution makes it hard. Here is a little trick to debug your programs. When you run a PyTorch program using CUDA operations, \n",
    "# the program usually doesn't wait until the computation finishes but continues to throw instructions at the GPU until it actually needs \n",
    "# a result (e.g. to evaluate using .item() or .cpu() or printing).\n",
    "# While this behaviour is key to the blazing performance of PyTorch programs, there is a downside: When a cuda operation fails, your program \n",
    "# has long gone on to do other stuff. The usual symptom is that you get a very non-descript error at a more or less random place somewhere \n",
    "# after the instruction that triggered the error.\n",
    "# One option in debugging is to move things to CPU. But often, we use libraries or have complex things where that isn't an option. So what now? If we could only get a good traceback, we should find the problem in no time.\n",
    "# This is how to get a good traceback:You can launch the program with the environment variable CUDA_LAUNCH_BLOCKING set to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loging in Weights & Biases (W&B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:32:47.448816Z",
     "iopub.status.busy": "2021-12-02T16:32:47.448596Z",
     "iopub.status.idle": "2021-12-02T16:33:13.532119Z",
     "shell.execute_reply": "2021-12-02T16:33:13.531063Z",
     "shell.execute_reply.started": "2021-12-02T16:32:47.448792Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb  # Machine learning experiment tracking, dataset versioning, and model evaluation Weights & Biases is the machine learning platform \n",
    "# for developers to build better models faster.\n",
    "\n",
    "\n",
    "wandb.init(project=\"my-test-project\", entity=\"doom84\")  # Enter your data here (your username and project name)\n",
    "# When you run this code, you will be prompted to enter API key\n",
    "\n",
    "# Weights & Biases connection error handler\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\n Get your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:13.53532Z",
     "iopub.status.busy": "2021-12-02T16:33:13.534934Z",
     "iopub.status.idle": "2021-12-02T16:33:13.544563Z",
     "shell.execute_reply": "2021-12-02T16:33:13.543351Z",
     "shell.execute_reply.started": "2021-12-02T16:33:13.535275Z"
    }
   },
   "outputs": [],
   "source": [
    "def id_generator(size=12, chars=string.ascii_lowercase + string.digits):  # this function Takes random choices from 12 ascii etters and digits\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))  # returns the resulting 12 character string\n",
    " \n",
    "# In Python, string ascii_lowercase will give the lowercase letters ‘abcdefghijklmnopqrstuvwxyz’.\n",
    "HASH_NAME = id_generator(size=12)  # will create a test variable and check how our generation function works\n",
    "print(HASH_NAME)  # display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:13.547064Z",
     "iopub.status.busy": "2021-12-02T16:33:13.546176Z",
     "iopub.status.idle": "2021-12-02T16:33:20.374251Z",
     "shell.execute_reply": "2021-12-02T16:33:20.37334Z",
     "shell.execute_reply.started": "2021-12-02T16:33:13.547033Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the configuration of our model\n",
    "CONFIG = {\"seed\": 42, # 2021->42\n",
    "          \"epochs\": 3, # 3->5->10->7->3\n",
    "          \"model_name\": \"roberta-base\",\n",
    "          \"train_batch_size\": 32,\n",
    "          \"valid_batch_size\": 64,\n",
    "          \"max_length\": 128,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-6,\n",
    "          \"T_max\": 500,\n",
    "          \"weight_decay\": 1e-6,\n",
    "          \"n_fold\": 10, #5->10->5->10\n",
    "          \"n_accumulate\": 1,\n",
    "          \"num_classes\": 1,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          \"hash_name\": HASH_NAME\n",
    "          }\n",
    "\n",
    "# 10folds * 3epochs * 13 minuts = 390 minuts = 6,5 hours  on GPU kaggle\n",
    "# if Using GPU: Tesla K80 google colab it will turn out many times faster = 1,5 часа\n",
    "\n",
    "CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "CONFIG['group'] = f'{HASH_NAME}-Baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:20.375764Z",
     "iopub.status.busy": "2021-12-02T16:33:20.3755Z",
     "iopub.status.idle": "2021-12-02T16:33:20.390077Z",
     "shell.execute_reply": "2021-12-02T16:33:20.388849Z",
     "shell.execute_reply.started": "2021-12-02T16:33:20.375739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sets the seed of the entire notebook so results are the same every time we run.\n",
    "# This is for REPRODUCIBILITY\n",
    "def set_seed(seed=42): \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:20.391838Z",
     "iopub.status.busy": "2021-12-02T16:33:20.391552Z",
     "iopub.status.idle": "2021-12-02T16:33:20.977504Z",
     "shell.execute_reply": "2021-12-02T16:33:20.976791Z",
     "shell.execute_reply.started": "2021-12-02T16:33:20.391785Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/validation_data.csv\")  # create a variable dataframe containing data from the original competition data file  \n",
    "print(df.shape)  # display statistics for in this file\n",
    "df.head()  # display the first 5 rows of the dataframe table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:20.979746Z",
     "iopub.status.busy": "2021-12-02T16:33:20.979146Z",
     "iopub.status.idle": "2021-12-02T16:33:21.043614Z",
     "shell.execute_reply": "2021-12-02T16:33:21.042763Z",
     "shell.execute_reply.started": "2021-12-02T16:33:20.979706Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])  # set the parameters for splitting our dataframe into data for training and testing\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):  # dataframe splitting\n",
    "    df.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "df[\"kfold\"] = df[\"kfold\"].astype(int)  # add one more column of folder number to the original dataframe\n",
    "df.head()  # display the first 5 rows of the dataframe table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.045459Z",
     "iopub.status.busy": "2021-12-02T16:33:21.044931Z",
     "iopub.status.idle": "2021-12-02T16:33:21.058317Z",
     "shell.execute_reply": "2021-12-02T16:33:21.057212Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.04542Z"
    }
   },
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):  # create a JigsawDataset class\n",
    "    def __init__(self, df, tokenizer, max_length):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n",
    "        # set the class attributes\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.more_toxic = df['more_toxic'].values\n",
    "        self.less_toxic = df['less_toxic'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        more_toxic = self.more_toxic[index]\n",
    "        less_toxic = self.less_toxic[index]\n",
    "        inputs_more_toxic = self.tokenizer.encode_plus(\n",
    "                                more_toxic,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        inputs_less_toxic = self.tokenizer.encode_plus(\n",
    "                                less_toxic,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length'\n",
    "                            )\n",
    "        target = 1\n",
    "        \n",
    "        more_toxic_ids = inputs_more_toxic['input_ids']\n",
    "        more_toxic_mask = inputs_more_toxic['attention_mask']\n",
    "        \n",
    "        less_toxic_ids = inputs_less_toxic['input_ids']\n",
    "        less_toxic_mask = inputs_less_toxic['attention_mask']\n",
    "        \n",
    "        \n",
    "        return {  # returns the obtained values\n",
    "            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n",
    "            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n",
    "            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n",
    "            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.06201Z",
     "iopub.status.busy": "2021-12-02T16:33:21.061121Z",
     "iopub.status.idle": "2021-12-02T16:33:21.077031Z",
     "shell.execute_reply": "2021-12-02T16:33:21.076107Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.061963Z"
    }
   },
   "outputs": [],
   "source": [
    "class JigsawModel(nn.Module):  # create a JigsawModel class\n",
    "    def __init__(self, model_name):  # initialization of the class at the input of the dataframe, tokenizer, max_length\n",
    "        # set the class attributes\n",
    "        super(JigsawModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(768, CONFIG['num_classes'])\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = self.model(input_ids=ids,attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.drop(out[1])\n",
    "        outputs = self.fc(out)\n",
    "        return outputs  # returns the obtained values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "![](https://i.imgur.com/qYwVt8V.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.079616Z",
     "iopub.status.busy": "2021-12-02T16:33:21.078609Z",
     "iopub.status.idle": "2021-12-02T16:33:21.095137Z",
     "shell.execute_reply": "2021-12-02T16:33:21.094478Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.079416Z"
    }
   },
   "outputs": [],
   "source": [
    "def criterion(outputs1, outputs2, targets):\n",
    "    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.097432Z",
     "iopub.status.busy": "2021-12-02T16:33:21.097058Z",
     "iopub.status.idle": "2021-12-02T16:33:21.109929Z",
     "shell.execute_reply": "2021-12-02T16:33:21.109343Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.097384Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n",
    "        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n",
    "        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n",
    "        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = more_toxic_ids.size(0)\n",
    "\n",
    "        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n",
    "        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n",
    "        \n",
    "        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n",
    "        loss = loss / CONFIG['n_accumulate']\n",
    "        loss.backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.11137Z",
     "iopub.status.busy": "2021-12-02T16:33:21.110923Z",
     "iopub.status.idle": "2021-12-02T16:33:21.129129Z",
     "shell.execute_reply": "2021-12-02T16:33:21.12815Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.111337Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n",
    "        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n",
    "        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n",
    "        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "        \n",
    "        batch_size = more_toxic_ids.size(0)\n",
    "\n",
    "        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n",
    "        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n",
    "        \n",
    "        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.130858Z",
     "iopub.status.busy": "2021-12-02T16:33:21.130517Z",
     "iopub.status.idle": "2021-12-02T16:33:21.149723Z",
     "shell.execute_reply": "2021-12-02T16:33:21.148795Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.13082Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n",
    "    # To automatically log gradients\n",
    "    wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        \n",
    "        # Log the metrics\n",
    "        wandb.log({\"Train Loss\": train_epoch_loss})\n",
    "        wandb.log({\"Valid Loss\": val_epoch_loss})\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "            run.summary[\"Best Loss\"] = best_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"Loss-Fold-{fold}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved{sr_}\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.152356Z",
     "iopub.status.busy": "2021-12-02T16:33:21.151456Z",
     "iopub.status.idle": "2021-12-02T16:33:21.16934Z",
     "shell.execute_reply": "2021-12-02T16:33:21.167243Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.152313Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n",
    "    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.171714Z",
     "iopub.status.busy": "2021-12-02T16:33:21.171382Z",
     "iopub.status.idle": "2021-12-02T16:33:21.187004Z",
     "shell.execute_reply": "2021-12-02T16:33:21.186251Z",
     "shell.execute_reply.started": "2021-12-02T16:33:21.171674Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T16:33:21.188842Z",
     "iopub.status.busy": "2021-12-02T16:33:21.188531Z"
    }
   },
   "outputs": [],
   "source": [
    "for fold in range(0, CONFIG['n_fold']):\n",
    "    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n",
    "    run = wandb.init(project='Jigsaw', \n",
    "                     config=CONFIG,\n",
    "                     job_type='Train',\n",
    "                     group=CONFIG['group'],\n",
    "                     tags=['roberta-base', f'{HASH_NAME}', 'margin-loss'],\n",
    "                     name=f'{HASH_NAME}-fold-{fold}',\n",
    "                     anonymous='must')\n",
    "    \n",
    "    # Create Dataloaders\n",
    "    train_loader, valid_loader = prepare_loaders(fold=fold)\n",
    "    \n",
    "    model = JigsawModel(CONFIG['model_name'])\n",
    "    model.to(CONFIG['device'])\n",
    "    \n",
    "    # Define Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = fetch_scheduler(optimizer)\n",
    "    \n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                                  device=CONFIG['device'],\n",
    "                                  num_epochs=CONFIG['epochs'],\n",
    "                                  fold=fold)\n",
    "    \n",
    "    run.finish()\n",
    "    \n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
